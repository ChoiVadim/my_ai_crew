# src/memory/memory_store.py
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from config.settings import settings
import os
from datetime import datetime
import uuid
from src.utils.metrics_logger import MetricsLogger
from src.utils.logger import get_logger
import time

logger = get_logger("memory.store")

class MemoryStore:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç—å—é –∞–≥–µ–Ω—Ç–∞"""
    
    def __init__(self, persist_directory=None, metrics_logger=None):
        self.metrics_logger = metrics_logger
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        persist_directory = persist_directory or settings.MEMORY_DIR
        os.makedirs(persist_directory, exist_ok=True)
        
        # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º embeddings —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –¥–µ—à–µ–≤—É—é –º–æ–¥–µ–ª—å
            openai_api_key=settings.OPENAI_API_KEY
        )
        self.persist_directory = persist_directory
        
        # ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Chroma —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ LangChain
        # –°–æ–≥–ª–∞—Å–Ω–æ docs: Chroma(collection_name, embedding_function, persist_directory)
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.vectorstore = Chroma(
                collection_name="memory_store",
                embedding_function=self.embeddings,
                persist_directory=persist_directory
            )
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–ª–µ–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç–∞—è
            # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞—è, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ - –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º
        except Exception:
            # –ï—Å–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é
            # Chroma –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—Å—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é –ø—Ä–∏ –ø–µ—Ä–≤–æ–º add_documents
            self.vectorstore = Chroma(
                collection_name="memory_store",
                embedding_function=self.embeddings,
                persist_directory=persist_directory
            )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=settings.MEMORY_CHUNK_SIZE,
            chunk_overlap=settings.MEMORY_CHUNK_OVERLAP
        )
    def set_metrics_logger(self, metrics_logger: MetricsLogger):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å metrics_logger –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —ç–∫–∑–µ–º–ø–ª—è—Ä–∞"""
        self.metrics_logger = metrics_logger

    def retrieve_memories_with_scores(self, query: str, k: int = 5):
        """–ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏ —Å–æ scores"""
        logger.info(f"üîç RAG SEARCH: Starting retrieval | query='{query[:100]}' | k={k}")
        start_time = time.time()
        
        try:
            if hasattr(self.vectorstore, 'similarity_search_with_score'):
                logger.debug("Using similarity_search_with_score method")
                results = self.vectorstore.similarity_search_with_score(
                    query=query,
                    k=k
                )
                
                memories = []
                confidence_scores = []
                sources = set()
                
                for doc, score in results:
                    memories.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "relevance_score": float(score)
                    })
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º distance –≤ confidence score
                    # Chroma –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (0-2), –≥–¥–µ 0 = –∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ confidence: —á–µ–º –º–µ–Ω—å—à–µ distance, —Ç–µ–º –≤—ã—à–µ confidence
                    distance = float(score)
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º: distance 0 -> confidence 1.0, distance 2 -> confidence 0.0
                    confidence = max(0.0, min(1.0, 1.0 - (distance / 2.0)))
                    confidence_scores.append(confidence)
                    if doc.metadata.get("category"):
                        sources.add(doc.metadata["category"])
                
                retrieval_latency = time.time() - start_time
                
                logger.info(f"‚úÖ RAG SEARCH: Found {len(memories)} results | latency={retrieval_latency:.3f}s | "
                          f"avg_confidence={sum(confidence_scores)/len(confidence_scores) if confidence_scores else 0:.3f} | "
                          f"source_diversity={len(sources)}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é
                for i, (mem, conf) in enumerate(zip(memories, confidence_scores), 1):
                    logger.info(f"\nüìÑ RETRIEVED CHUNK {i} FROM RAG:")
                    logger.info(f"  Confidence Score: {conf:.4f}")
                    logger.info(f"  Distance Score: {mem['relevance_score']:.4f}")
                    logger.info(f"  Category: {mem['metadata'].get('category', 'N/A')}")
                    logger.info(f"  Timestamp: {mem['metadata'].get('timestamp', 'N/A')}")
                    logger.info(f"  Content length: {len(mem['content'])}")
                    logger.info(f"  Full Content:")
                    logger.info(f"  {mem['content']}")
                    logger.info("-"*80)
                
                # ‚úÖ –í–ê–ñ–ù–û: –õ–æ–≥–∏—Ä—É–µ–º RAG –º–µ—Ç—Ä–∏–∫–∏ –í–°–ï–ì–î–ê, –¥–∞–∂–µ –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç
                if self.metrics_logger:
                    # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω confidence score –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                    if not confidence_scores:
                        confidence_scores = [0.0]  # –ú–∏–Ω–∏–º—É–º –æ–¥–∏–Ω score –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    
                    self.metrics_logger.log_rag_metrics(
                        retrieval_confidence_scores=confidence_scores,
                        num_chunks_retrieved=len(memories),
                        source_diversity=len(sources),
                        retrieval_latency=retrieval_latency,
                        metadata={"query": query[:100], "k": k, "found_results": len(memories) > 0, "method": "similarity_search_with_score"}
                    )
                else:
                    logger.warning("metrics_logger is None - RAG metrics will not be logged!")
                
                return memories
            else:
                # ‚úÖ –ï—Å–ª–∏ –Ω–µ—Ç similarity_search_with_score, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫, –Ω–æ —Ç–æ–∂–µ –ª–æ–≥–∏—Ä—É–µ–º
                logger.debug("Using similarity_search fallback method")
                results = self.vectorstore.similarity_search(
                    query=query,
                    k=k
                )
                
                memories = []
                sources = set()
                
                for doc in results:
                    memories.append({
                        "content": doc.page_content,
                        "metadata": doc.metadata,
                        "relevance_score": 1.0  # –ù–µ—Ç score, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    })
                    if doc.metadata.get("category"):
                        sources.add(doc.metadata["category"])
                
                retrieval_latency = time.time() - start_time
                
                logger.info(f"‚úÖ RAG SEARCH: Found {len(memories)} results (fallback method) | latency={retrieval_latency:.3f}s | "
                          f"source_diversity={len(sources)}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —á–∞–Ω–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é
                for i, mem in enumerate(memories, 1):
                    logger.info(f"\nüìÑ RETRIEVED CHUNK {i} FROM RAG (fallback):")
                    logger.info(f"  Category: {mem['metadata'].get('category', 'N/A')}")
                    logger.info(f"  Timestamp: {mem['metadata'].get('timestamp', 'N/A')}")
                    logger.info(f"  Content length: {len(mem['content'])}")
                    logger.info(f"  Full Content:")
                    logger.info(f"  {mem['content']}")
                    logger.info("-"*80)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–∞–∂–µ –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                if self.metrics_logger:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π confidence score 0.5 –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∑ score
                    confidence_scores = [0.5] * len(memories) if memories else [0.0]
                    
                    self.metrics_logger.log_rag_metrics(
                        retrieval_confidence_scores=confidence_scores,
                        num_chunks_retrieved=len(memories),
                        source_diversity=len(sources),
                        retrieval_latency=retrieval_latency,
                        metadata={"query": query[:100], "k": k, "method": "similarity_search", "found_results": len(memories) > 0}
                    )
                else:
                    logger.warning("metrics_logger is None - RAG metrics will not be logged!")
                
                return memories
                
        except Exception as e:
            retrieval_latency = time.time() - start_time
            if self.metrics_logger:
                self.metrics_logger.log_rag_metrics(
                    retrieval_confidence_scores=[],
                    num_chunks_retrieved=0,
                    source_diversity=0,
                    retrieval_latency=retrieval_latency,
                    metadata={"error": str(e), "error_type": type(e).__name__, "query": query[:100], "k": k}
                )
            else:
                print("‚ö†Ô∏è WARNING: metrics_logger is None when trying to log error metrics!")
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –ø–∞–º—è—Ç–∏: {e}")
            return []
            
    def save_memory(self, content: str, metadata: dict = None):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø–∞–º—è—Ç—å
        
        Args:
            content: –¢–µ–∫—Å—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        
        Returns:
            str: –°–æ–æ–±—â–µ–Ω–∏–µ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        category = metadata.get("category", "unknown") if metadata else "unknown"
        logger.info(f"üíæ SAVING MEMORY: category={category} | content_length={len(content)}")
        
        timestamp = datetime.now().isoformat()
        
        default_metadata = {
            "timestamp": timestamp,
            "type": "memory"
        }
        if metadata:
            default_metadata.update(metadata)
        
        # ‚úÖ –°–æ–∑–¥–∞–µ–º Document —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        doc = Document(page_content=content, metadata=default_metadata)
        
        # ‚úÖ –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        documents = self.text_splitter.split_documents([doc])
        logger.info(f"üì¶ Split into {len(documents)} chunks")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —á–∞–Ω–∫, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω
        for i, chunk_doc in enumerate(documents, 1):
            logger.info(f"\nüíæ CHUNK {i} TO SAVE:")
            logger.info(f"  Category: {chunk_doc.metadata.get('category', 'N/A')}")
            logger.info(f"  Timestamp: {chunk_doc.metadata.get('timestamp', 'N/A')}")
            logger.info(f"  Content length: {len(chunk_doc.page_content)}")
            logger.info(f"  Content:")
            logger.info(f"  {chunk_doc.page_content}")
            logger.info("-"*80)
        
        # ‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        # –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: add_documents(documents, ids)
        document_ids = [str(uuid.uuid4()) for _ in documents]
        
        # ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ LangChain
        # docs: vector_store.add_documents(documents=[doc1, doc2], ids=["id1", "id2"])
        try:
            self.vectorstore.add_documents(
                documents=documents,
                ids=document_ids
            )
            # Chroma –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ persist_directory
            result = f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(documents)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ø–∞–º—è—Ç—å"
            logger.info(f"‚úÖ MEMORY SAVED: {len(documents)} chunks successfully stored | category={category}")
            return result
        except Exception as e:
            logger.error(f"‚ùå MEMORY SAVE ERROR: {type(e).__name__} | error={str(e)}", exc_info=True)
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {str(e)}"
    
    def retrieve_memories(self, query: str, k: int = 5):
        """
        –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
        
        Returns:
            list: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è—Ö
        """
        try:
            # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º similarity_search —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            # docs: similar_docs = vector_store.similarity_search("your query here", k=3)
            results = self.vectorstore.similarity_search(
                query=query,
                k=k
            )
            
            # –ï—Å–ª–∏ –Ω—É–∂–Ω—ã —Ç–∞–∫–∂–µ scores, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å similarity_search_with_score
            # –ù–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥
            
            memories = []
            for doc in results:
                memories.append({
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "relevance_score": 1.0  # similarity_search –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç score
                })
            
            return memories
            
        except Exception as e:
            # –ï—Å–ª–∏ –ë–î –ø—É—Å—Ç–∞—è –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –ø–∞–º—è—Ç–∏: {e}")
            return []
    
    def delete_memory(self, ids: list):
        """
        –£–¥–∞–ª–∏—Ç—å –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ ID
        
        Args:
            ids: –°–ø–∏—Å–æ–∫ ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        
        Returns:
            bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ
        """
        try:
            # ‚úÖ –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: vector_store.delete(ids=["id1"])
            self.vectorstore.delete(ids=ids)
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏: {e}")
            return False
    
    def get_all_memories(self, limit: int = 100):
        """
        –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            dict: –°–ª–æ–≤–∞—Ä—å —Å ids, documents, metadatas
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            all_docs = self.vectorstore.get(limit=limit)
            return all_docs
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –≤—Å–µ—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π: {e}")
            return {"ids": [], "documents": [], "metadatas": []}
    
    def clear_all_memories(self):
        """
        –û—á–∏—Å—Ç–∏—Ç—å –≤—Å—é –ø–∞–º—è—Ç—å
        
        Returns:
            bool: True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–æ
        """
        try:
            all_ids = self.get_all_memories()["ids"]
            if all_ids:
                self.delete_memory(all_ids)
            return True
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –ø–∞–º—è—Ç–∏: {e}")
            return False